name: Build + Deploy (Staging)

on:
  push:
    branches:
      - master
    tags:
      - "v*"

permissions:
  contents: read
  packages: write
  id-token: write

env:
  AWS_REGION: us-west-2
  IMAGE_NAME: calibratehealth
  GHCR_IMAGE: ghcr.io/${{ github.repository_owner }}/calibratehealth

jobs:
  build_and_push:
    name: Build and Push Image (GHCR + ECR)
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure AWS credentials (ECR push)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN_BUILD }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        id: ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Compute tags
        id: meta
        shell: bash
        run: |
          set -euo pipefail

          ECR_IMAGE="${{ steps.ecr.outputs.registry }}/${IMAGE_NAME}"
          # Docker image refs require lowercase repository names (GH owner can be mixed-case).
          GHCR_IMAGE="${GHCR_IMAGE,,}"

          TAGS=()
          TAGS+=("${GHCR_IMAGE}:sha-${GITHUB_SHA}")
          TAGS+=("${ECR_IMAGE}:sha-${GITHUB_SHA}")

          if [[ "${GITHUB_REF}" == "refs/heads/master" ]]; then
            # Tag a human-friendly branch alias for convenience.
            TAGS+=("${GHCR_IMAGE}:${GITHUB_REF_NAME}")
            TAGS+=("${ECR_IMAGE}:staging")
          fi

          if [[ "${GITHUB_REF_TYPE}" == "tag" ]]; then
            TAGS+=("${GHCR_IMAGE}:${GITHUB_REF_NAME}")
          fi

          {
            echo "ecr_image=${ECR_IMAGE}"
            echo "ghcr_image=${GHCR_IMAGE}"
            echo "tags<<EOF"
            printf "%s\n" "${TAGS[@]}"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"

      - name: Build and push (multi-arch)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile.app
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}

  deploy_staging:
    name: Deploy Staging via SSM
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: build_and_push
    if: github.ref == 'refs/heads/master'

    steps:
      - name: Configure AWS credentials (staging deploy)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN_DEPLOY_STAGING }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve staging instance ID
        id: instance
        shell: bash
        run: |
          set -euo pipefail

          INSTANCE_ID="$(aws ec2 describe-instances \
            --filters \
              "Name=tag:App,Values=${IMAGE_NAME}" \
              "Name=tag:Environment,Values=staging" \
              "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].InstanceId" \
            --output text)"

          if [[ -z "${INSTANCE_ID}" || "${INSTANCE_ID}" == "None" ]]; then
            echo "Unable to find a running staging instance (tag App=${IMAGE_NAME}, Environment=staging)." >&2
            exit 1
          fi

          echo "instance_id=${INSTANCE_ID}" >> "$GITHUB_OUTPUT"

      - name: Run deploy command
        id: deploy
        shell: bash
        run: |
          set -euo pipefail

          INSTANCE_ID="${{ steps.instance.outputs.instance_id }}"
          COMMAND_ID="$(aws ssm send-command \
            --instance-ids "${INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --comment "Deploy staging (${GITHUB_SHA})" \
            --parameters commands=["/opt/calibratehealth/deploy.sh"] \
            --query "Command.CommandId" \
            --output text)"

          echo "command_id=${COMMAND_ID}" >> "$GITHUB_OUTPUT"

          echo "SSM deploy command id: ${COMMAND_ID} (instance: ${INSTANCE_ID})"

          # The waiter exits non-zero on Failed/TimedOut/Cancelled, which prevents us from
          # fetching stdout/stderr. Always fetch the invocation output and fail explicitly.
          aws ssm wait command-executed --command-id "${COMMAND_ID}" --instance-id "${INSTANCE_ID}" || true

          STATUS="$(aws ssm get-command-invocation \
            --command-id "${COMMAND_ID}" \
            --instance-id "${INSTANCE_ID}" \
            --query 'Status' \
            --output text)"

          RESPONSE_CODE="$(aws ssm get-command-invocation \
            --command-id "${COMMAND_ID}" \
            --instance-id "${INSTANCE_ID}" \
            --query 'ResponseCode' \
            --output text)"

          echo "SSM status: ${STATUS} (response_code: ${RESPONSE_CODE})"

          echo "::group::SSM stdout"
          aws ssm get-command-invocation \
            --command-id "${COMMAND_ID}" \
            --instance-id "${INSTANCE_ID}" \
            --query 'StandardOutputContent' \
            --output text
          echo "::endgroup::"

          echo "::group::SSM stderr"
          aws ssm get-command-invocation \
            --command-id "${COMMAND_ID}" \
            --instance-id "${INSTANCE_ID}" \
            --query 'StandardErrorContent' \
            --output text
          echo "::endgroup::"

          if [[ "${STATUS}" != "Success" ]]; then
            echo "SSM deploy failed with status: ${STATUS}" >&2
            exit 1
          fi
